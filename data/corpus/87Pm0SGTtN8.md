# Marc Andreessen on AI, the Future of Work, and the Historic Moment We're Living Through

_A conversation on Lenny's Podcast_

## Introduction

**Opening Thoughts**

If we didn't have AI, we'd be in a panic right now about what's going to happen to the economy. We've actually been in a regime for 50 years of very slow technological change in the face of declining population growth. The timing has worked out miraculously well. We're going to have AI and robots precisely when we actually need them. The remaining human workers are going to be at a premium, not at a discount.

AI is the philosopher's stone. Now we have a technology that transfers the most common thing in the world—sand—into the most rare thing in the world—thought.

---

**Lenny:** Today my guest is Marc Andreessen, one of the most seminal figures in tech and business. He invented the web browser, built the world's largest venture firm, and is a multi-time founder and investor in essentially every generational tech company. He's also one of the most clear-minded, lateral, and insightful thinkers about both the past and the future of technology.

In this conversation, we discuss how unique and significant the moment we're all living through right now is, what skills he's teaching his kids to thrive in the AI future, what happens to product managers, designers, and engineers in the coming years, where moats exist in AI, what the most AI-native founders are doing differently, and much more.

## The Historic Nature of This Moment

**Lenny:** How big of a deal is the moment in time that we are living through right now?

**Marc:** This is a very, very historic time. I think 2025 was maybe the most interesting year in my entire career and probably life, and I would expect 2026 to exceed that.

I've seen some stuff. It feels like two things are happening. One is the trust that a lot of people have had in legacy institutions around the world is in full-scale collapse right now. There's a lot of data to support that. There are structures, orders, and institutions that people have relied on for a long time that have proven not to be up for the challenge.

Corresponding with that is that the national and global conversation has become liberated. This incredible revolution in freedom of speech and freedom of thought—the ability for people to openly discuss things that maybe they couldn't discuss even a few years ago—has dramatically expanded. That's now on a one-way train for a much broader range of discourse.

There are also incredibly massive geopolitical shifts happening. The US is changing a lot, Europe is changing a lot, China is changing a lot, Latin America is changing a lot. A lot of assumptions are being pulled out into the daylight and re-examined.

The fact that all these things are happening at the same time—all of these countries and industries in upheaval, AI as this new technology that's going to affect things, and citizens being able to fully participate and argue things out. Those three big mega things are colliding at the same time. We're probably just at the very beginning of all three of those. They all feel like historical moment shifts, comparable in magnitude to the fall of the Berlin Wall in 1989 or the end of World War II.

## What's Not Being Priced In: The Deeper Context

**Lenny:** What do you think isn't being priced in yet in terms of the impact AI is going to have?

**Marc:** At this point, it's pretty clear with our technology hats on that this stuff is really working now. Three years ago—and it was only three years ago—there was the ChatGPT moment. The big question was: this is incredibly fun and creative, we have machines that can compose Shakespeare and rap lyrics, but can you harness this technology for reasoning and problem-solving in domains that really matter—medicine, science, law?

It turns out the answer is yes. The last 12 months, especially the last three months, have proven that AI can really do it. You're seeing it now—AI is developing new math theorems. Over the holiday break, the AI coding thing really hit critical mass. The world's best programmers, for the first time, basically said AI is now coding better than we can. That's incredibly powerful.

I think we can assume that AI is going to get really good at reasoning in any domain in which there are verifiable answers. That's going to include many very important domains.

### The Missing Context: Slow Technological Progress

But I think what's not well understood is that a lot of people have this one-dimensional view: the technology works, so AI just sweeps the world and changes everything. I think that's the wrong frame. It's based on an incomplete understanding of the world we've been living in for the last 80 years.

It has felt to us like in the US and the West for the last 30 or 50 years that we've been in a time of great technological change. But if you look for statistical evidence of that, you basically can't find it.

Economists measure the rate of technological change in the economy through productivity growth. For the last 50 years, productivity growth has actually been very low, not very high. We all feel like it's been high, but it's actually been very low. The pace of productivity growth in the US is running at about half of what it ran at between 1940 and 1970, and about a third of the pace between 1870 and 1940.

Statistically, in the US and the West, technology's impact on the economy has actually slowed way down. AI is going to hit an environment in which we have had almost no technological progress in the actual economy for a very long time.

### The Demographic Collapse

Then there's this other incredible thing happening: the demographic collapse. It's a Western phenomenon and increasingly a global phenomenon. The rate of reproduction of the human species is in rapid decline. There are many countries, including the US, where the rate of reproduction is under two, meaning many countries around the world—including China, which is a really big deal—are actually going to depopulate over the next century.

You have this precondition that says there's actually been very little technological progress happening in the world, and the world is going to depopulate. AI is going to enter a world in which those two things are true. This is incredibly important because we actually need AI to work in order to get productivity growth up, which is what we need to get economic growth up. We actually need AI to work because we're going to need machines to do all the jobs that we're not going to have people to do.

The interplay of these factors is going to be much more interesting and frankly more complex than a lot of people have been thinking.

## Teaching Kids for the AI Future

**Lenny:** I know you have a kid. What are you teaching them? What skills are you steering them towards?

**Marc:** We have a 10-year-old, and we homeschool, so we think a lot about this.

I think it's pretty clear that AI is going to take people who are good at doing things and make them very good at doing things. It's going to be a tool that raises the average across the board. Anybody who's in a position where they need to write something, design something, or write code—if they're pretty good at it today and they use AI, all of a sudden they're very good at it.

But there's this other thing happening, which we're really seeing particularly in coding right now: the really great people are becoming spectacularly great. The superpowered individual. If you're very good at coding and you can really harness AI, you can become spectacularly great and super productive.

My friends who are really good coders are like, "Oh my god, all of a sudden I'm not twice as good as I used to be. I'm like 10 times as good as I used to be."

At the unit of an individual kid, the question is how do you get them in a position where they're this superpowered individual such that they're going to be really deep in whatever it is they're going to do, but they're going to be deep in a way that lets them fully use the power of AI to be not just great, but spectacularly great. That's the real opportunity. That's what we're shooting for and what I would encourage parents to shoot for.

### Agency: The Willingness to Do Things

**Lenny:** So what I heard there is essentially agency, building them not waiting for someone to tell them what to do.

**Marc:** Yeah, this term "agency" that's become very popular in California for the last couple years—I had a lot of trouble with it early on because I'm like, "Agency? What are they talking about?" What they're talking about is initiative, willingness to do things. The demo bird has the great term "live player"—you can be a primary participant in events.

At first I was like, well yeah, that's kind of obvious. But then I realized it's not so obvious anymore because so much of our society is based on "there are all these rules and everybody gets taught you're supposed to follow all these rules." If you break the rules, everybody gets freaked out.

We have somehow worked our way into a state where the natural assumption for a lot of people is that you want to train kids to follow all the rules. The school system has gotten more and more focused on that over time.

I just had this conversation with my 10-year-old last night. I rolled out the concept of "in order to lead, you must first learn to obey. In order to issue orders, you must learn how to follow orders." You try to keep him with some level of structure in his life, not just pure agency.

Some rules are important, but there is a huge premium in life on being somebody who is able to fully take responsibility for things, fully take charge, run an organization, lead a project, create something new. That has been maybe a little bit diminished in our culture over the last 30 years. It's healthy that there's now a term for that that's coming back into vogue.

AI should be the ultimate lever on the world for a kid with agency to be able to say, "I can actually be a primary contributor in everything from developing new areas of physics to writing code to being an artist to writing novels. I can fully participate in the world. I can really change things." That combination of that idea with this technology feels very healthy to me.

### AI as the Philosopher's Stone

**Lenny:** What is that quote about "Give me a lever and I'll move the world"?

**Marc:** Yeah, that's exactly right. It's actually funny you mention that. The early scientists, including Isaac Newton, were super obsessed with alchemy. Newton developed Newtonian physics and calculus and all these things, but the thing he was really obsessed with was alchemy, which he could never get to work.

Alchemy was the transmutation of lead into gold—the transmutation of something very common (lead) into something very rare and valuable (gold). He spent decades trying to figure out the philosopher's stone, which would be the machine or process that would transmute the common thing into the rare thing. He never figured it out, and it was incredibly frustrating. Nobody ever figured that out.

Now we literally have, with AI, a technology that transfers sand into thought—the most common thing in the world converted into the most rare thing in the world. AI is the philosopher's stone. It actually is that, and it's this incredibly powerful tool.

## The Future of Product Managers, Engineers, and Designers

**Lenny:** What's your sense of the future of these three very specific roles: product manager, engineer, designer?

**Marc:** This is a really funny question. These three roles are the central roles for building in tech companies. The way I've been describing it is: you know the concept of the Mexican standoff? The movie scene where two guys have guns pointing at each other's heads? In John Woo movies, he loves the three-way Mexican standoff where you've got a triangle of people, each aiming at the other two.

There's like a Mexican standoff happening between those three roles. Every coder now believes they can also be a product manager and a designer because they have AI. Every product manager thinks they can be a coder and a designer. And then every designer knows they can be a product manager and a coder.

People in each of those roles now know or believe that with AI they don't need the other two roles anymore—they can have AI do that. And of course, all three of them are going to realize that AI can also be a better manager, so they're going to end up aiming the guns up the org chart. But that's the next phase.

What I think is so fascinating about this Mexican standoff is they're actually all kind of correct. AI is actually now a really good coder, a really good designer, and also a really good product manager. It's actually good at doing all three of those things, or at least doing a lot of the tasks involved in those three jobs.

### The Superpowered Individual

This goes back to the idea of the superpowered individual. If I'm a coder, step one is I need to make sure that I really understand AI coding and how coding is going to change in the future. I need to go from being a coder who writes code entirely by hand to being a coder who orchestrates a dozen instances of coding bots.

But the other part of it is: How do I become that superpowered individual? How do I become a coder that also harnesses AI so that I can also be a great product manager and I can also be a great designer? The same thing for the product manager—how do I make sure I can now use coding tools, do AI-based design? And the same thing for the designer.

Maybe those individual roles change. Maybe those are not anymore sort of stovepipe roles the way they have been for the last 30 years. But what happens is the talented people in any of those roles become superpowered and become good at doing all three of those things. Those people become incredibly valuable because they can actually build and design new products from scratch, which is the most valuable thing.

### Jobs vs. Tasks

Economists talk about this: there's the concept of the job, but the job is not the atomic unit of what happens in the workplace. The atomic unit is the task. A job is a bundle of tasks.

Everybody wants to talk about job loss, but really what you want to look at is task loss—tasks changing.

The classic example: once upon a time, executives never used typewriters or personal computers themselves. If you were a vice president of a company in 1970, you did not have a typewriter or computer on your desk. You had a secretary who you dictated memos to.

Then emails started to show up. The job of the secretary changed from sending out letters with stamps to sending or receiving emails with other admins. The secretary would print out the email and bring it into the executive's office. The executive would read the email on paper, scrawl the reply, and give that message back to the secretary who would type it into the computer and send it as an email.

Fast forward to today: none of that happens. Now executives just do all their own email. They still have secretaries or admins, but they're doing different tasks—travel planning, orchestrating events, all these other things. The task set of the executive has actually expanded to do more of the clerical work themselves—sit there and type their own memos, which 50 years ago they never would have done.

The executive job still exists. The secretary job still exists. But the tasks have changed.

That's what's going to happen in coding, product management, and design. The job persists longer than the individual tasks. As the tasks change enough, that's when the jobs change.

At the level of an individual, you want to think: I have this job, the job is a bundle of tasks, I need to be really good at making sure I can swap the tasks out. I can really adapt, use the new technology, get really good at AI coding. Then I want to add skills. I can also get really good at design. I can also get really good at product management because I've got this new tool.

Ten years from now, is your job title coder, coder-designer-product-manager, or is it just "I build products" or "I tell the AI how to build products"? Whatever that job is called, who even knows, but it's going to be incredibly important because the people doing that job are going to be orchestrating the AI. That's the track that the best people are going to be on.

### The Evolution of Programming

People aren't fully grasping how much software engineering is changing. It's pretty clear we're going to be in a world soon where engineers are not actually writing code, which a year ago we would not have thought. Now it's clearly where it's heading. There's going to be this artisanal experience of sitting there writing code.

Let me give a history lesson. Do you know the original definition of the term "calculator"?

**Lenny:** No.

**Marc:** It referred to people. Before there were electronic calculators or computers, the way that you would do computing—the way an insurance company would calculate actuarial tables or the military would calculate troop logistics—you would actually have a room full of people. You could have hundreds or thousands or tens of thousands of people doing this. You would have somebody at the head of the room responsible for the mathematical equation, and they would parcel out the individual mathematical calculations to people sitting at desks doing them all by hand. That job title was "calculators."

We've gone from a world where you literally have people doing mathematical equations by hand. Then we got the first computers. The first computers didn't have programming languages—they only had machine code. So the first computers were programmed with ones and zeros, then that became punch cards. Then you got assembly language, which was machine code but with some level of English added to it.

When I was coming up, it was higher-level languages like C that compiled into machine code. I still remember when scripting languages—we developed JavaScript at Netscape, and then Python took off, and Perl—there was this big fight in the technical community: "Is scripting real programming or not?" Because real programmers write code that compiles to machine code. Real programmers do memory management themselves. These JavaScript or Python programmers are doing this kind of lightweight thing—does it even really count as coding?

Of course the answer is yes, it very much counted, and now most coding is done with scripting languages. The scripting languages have abstracted away five layers of detail that people used to do by hand.

AI coding is the next layer. AI coding abstracts away the process of actually writing the scripting code. In one sense, this is a really big deal for all the obvious reasons. But on the other hand, this is the next layer of task redefinition under the job of programmer.

Now, if you talk to the world's best programmers today, they'll tell you: "Oh, my job is I'm sitting there and I'm orchestrating 10 coding bots that are running in parallel. I'm watching them, shifting from browser to browser or terminal to terminal. My day job now is arguing with the AI bots, trying to get them to write the right code, then debug it, fix the problems, change the spec."

Now the job of the programmer is to argue with the coding bots. But if you don't know how to write the code yourself, you don't know how to evaluate what the coding bots are giving you.

My 10-year-old is super into computers and programming. He's using Claude and ChatGPT and Copilot and all these things. He loves coding. He's on Replit all the time doing vibe coding, doing games. It's hysterical—he's a 10-year-old who spends two hours at dinner arguing with an AI for fun.

But what I'm telling him is: No, look, you need to still fully understand and learn how to write and understand code because the coding bots are giving you code. If it doesn't work or if it's not doing what you expect or it's not fast enough, you need to be able to understand the results of what the AI is giving you, in the same way that somebody writing scripting language code does need to understand ultimately how the microprocessor works.

It's this upleveling of capability where you actually want the depth to be able to go down and understand what the thing is actually doing, even if you're not spending your day actually doing that by hand.

Now programmers are going to be 10 times or 100 times or a thousand times more productive than they used to be. The tasks are definitely changing. The nature of the job is changing. But are human beings going to be involved in the coding process and overseeing the AI coding? The answer is of course, absolutely, 100%. No question.

### Still Learning to Code

**Lenny:** So you're in the camp of still learning to code, still a valuable skill.

**Marc:** Oh yeah, totally. Look, if you just want to put yourself on autopilot and you can't be bothered and you're just going to have AI write the code and it's going to generate whatever it does, if the goal is to be a mediocre coder, then just let the AI do it. It's fine. The AI is going to be perfectly good at generating infinite amounts of mediocre code. No problem.

If the goal is "I want to be one of the best software people in the world and I want to build new software products and technologies that really matter," then yeah, you 100% want to still be—you want to go all the way down. You want your skill set to go all the way down to assembly and machine code. You want to understand every layer of the stack. You want to deeply understand what's happening at the level of the chip and the network.

By the way, you also really deeply want to understand how the AI itself works, because people who understand how the AI works are clearly able to get more value out of it than somebody who doesn't understand how it works. You're always more productive if you know how the machine works when you use the machine.

The superpowered individual on the other end of this that wants to do great things with the new technology—yes, you 100% want to understand this thing all the way down the stack because you want to be able to understand what it's giving you. When something doesn't work or when something isn't right, you want to be able to really quickly understand why that is.

By the way, AI is your best friend at helping you learn all that. It's like, "Oh, this isn't fast enough. I need to figure out a different approach to memory management." You can be like, "AI, let's spend 10 minutes. Teach me how to do this. Teach me what this all means." All of a sudden, you have this incredibly synergistic relationship with the AI where it's also helping you get better at the same time as doing a lot of work for you.

### Design and Taste

**Lenny:** The other element I hear more and more is the skill of taste and design and user experience. That feels like a very hard skill to learn, which tells me design is going to be much more valuable in the future.

**Marc:** Yeah, that's right. At the task level, designing the perfect icon—the AI's going to do that all day long. It's going to give you a thousand icon designs. It's going to be great. There will still be some level of human icon design, but AI is going to get really good at that.

But what are we trying to do? The capital-D design of "all right, what is this thing for? How is this going to function in a world of human beings? What's going to make people happy when they use it? Is it going to make people feel good about themselves? Is it going to fit into the rest of their life? Is it going to challenge them in the right way?"—all these kinds of higher-level questions that the great designers have always thought about.

The job of designer will involve much more of those higher-level, more important components, with AI doing a lot more of the underlying tasks.

Think of the world's best designers—Johnny Ive or whatever. If I'm a designer today, if I'm a 25-year-old designer and I aspire to be Johnny Ive in a decade, all of a sudden I have a new path I can use to get there. Johnny Ive did everything he did without AI. A young designer can be like, "Wow, if I really harness AI in a decade, I'm going to be the best designer the world's ever seen because it's not just going to be me. It's going to be me plus being so superpowered by this technology to be able to do so much more. So much more of my time and attention is going to be focused on these higher-level things that most designers never get to."

### The T-Shaped Strategy (or E/F-Shaped)

**Lenny:** So maybe what I'm hearing here is kind of this T-shaped strategy: be very, very good at that specific role—product management, engineering, design—and then get good enough at these other two roles.

**Marc:** I think that's great. Scott Adams—unfortunately he just passed away, which is a real tragedy—he had this famous career advice. He used to say: "Look, I could have been a pretty good cartoonist or I could have been pretty good at business, but the fact that I was a cartoonist who understood business made me spectacularly great at making Dilbert."

Even the world's best cartoonist who didn't understand business could have never written Dilbert. And the world's best business people who didn't know how to do cartoons couldn't have done Dilbert. It took somebody who actually had both of those skills to be able to make Dilbert, one of the most successful cartoons in history.

The way Scott always described it was that the additive effect of being good at two things is more than double. The additive effect of being good at three things is more than triple. You become a super relevant specialist in the combination of the domains.

You see this all over the economy. In Hollywood, there are a lot of writers who can't direct a movie—they can be very successful writers. There are a lot of directors who can't write a movie—they can be very successful directors. But the superstars in the entertainment industry are the people who can write and direct. They have a term for those: they call them "auteurs." Those are the people who are the real creative forces that move the field.

Hollywood has the same Mexican standoff going on right now as we described in tech, except in Hollywood for film it's the director, the writer, and the actor. The director is now thinking, "I don't need the writer anymore because AI can write the script, and I don't need the actor anymore because I can have AI actors." The writer is saying, "I don't need the director because AI can direct the movie and AI can do the actors." And the actor is saying, "I don't need either one of these guys. I can have AI direct the thing, I can have AI write the thing, and I'm just going to show up and do my performance."

Again, what's great about it is they're all correct. Each person in each of those three fields is going to be able to expand laterally and pick up those other additional skills. As a consequence, you're going to have more people who can write and direct, or write and act, or direct and act, or do all three.

The T—if you think about the T configuration—the breadth at the top of the T is "how many individual domains are you familiar enough with to be able to use the AI tools to be able to do really good work?" And then the depth is "how deep can you go in at least one of those domains so that you really deeply know what you're doing?"

If you're super deep on coding and you can use AI to do design and you can use AI to do product management, that's your T right there. You're a triple threat at the top of the T, but with this level of technical grounding underneath. At that point, you're the superpowered individual. You're going to be able to perform feats of magic in terms of designing and building new products that people in my generation couldn't have even dreamed of.

I think this is a universal theory that can apply across the entire economy.

### Don't Be Fungible

**Lenny:** I'm going to invent a new framework right now. Forget the T framework. I'm picturing an F sideways or an E where there's two or three downward parts. What I'm hearing is get good at least two.

**Marc:** I think that's right. My friend Larry Summers had a different version of the Scott Adams thing. He used to tell people: "Don't be fungible." That's economics speak, and what that means essentially is don't be replaceable. Don't be a cog.

Don't just be one thing. If you're just a designer, just a product manager, just a coder, then in theory you can be swapped in or out. But if you have this E or F laying on its side, if you have this combination of things that's actually quite rare, then all of a sudden you're not fungible. Not only are you not fungible, you're actually massively important because you're one of the only people in the world who can actually do that combination of things.

Your ability now to become one of those people is just titanically enhanced with AI compared to anything we've ever seen before.

### AI Will Teach You

I can't overstress the following for anybody listening to this: The thing about AI that I think people are just not getting enough benefit out of yet is it will teach you.

This is amazing. There's never been a technology before where you can ask it "teach me how to do this thing." People spend too much time on "what am I going to try to get it to do for me?"—which is of course very important—but the other side of it is "what can I get it to teach me how to do?" It's just as good at that.

People who really want to improve themselves and develop their career should be spending every spare hour at this point talking to an AI being like, "All right, train me up. Superpower me. Tell me how to—I'm a coder, train me how to be a product manager." It will happily do that. It knows exactly how to do that. "Run me, make me problems, make me assignments, then evaluate my results." It will do that just as happily as it will do work for you.

Two tricks I've heard: One is to watch the output—what the agent is doing and thinking as it's doing the work. If you're not an engineer, just sit there and watch it think and make decisions. It's almost become this layer on top of learning to code—learning to see what the agent is doing and thinking because that teaches you about architecture.

The other is when you get stuck and then you figure out how to unstick yourself, you ask it: "What could I have done differently? What could I have said that would have avoided this error in the first place?"

If you ask an AI "write me this code" and it comes back and it doesn't work, if all you know is "I asked and it gave me back something that's not good," what do you even do with that? You don't understand why it gave you that result. You don't really understand what to tell it to try to get it to do something different.

But if you actually watch what it's doing, and if you have the grounding—that leg of your E or your F—then you can be like, "Oh, I see what it's doing. I see where it made the mistake. I see where it went sideways." Then you're all of a sudden able to intervene and able to say, "No, no, that's not what I meant. Do this other thing."

Everything I'm saying is also the same as if you're working with human beings. If you and I are colleagues and I ask you to do something and you come back with something completely different, I do need to understand what was happening in your head in order to be able to give you feedback. If I just tell you "that's wrong," nothing happens. I need to have theory of mind. I need to understand what you were thinking in order to really give you the right feedback.

The great thing with AI is AI will happily sit there and explain all day long why it's doing what it's doing. It'll happily critique itself. You can have one AI critique the other AI. You have one AI write the code, you have another AI debug the code, and you can play the AIs off against each other and get them to argue with each other.

These are all the kinds of skills that are going to become incredibly valuable.

## AI-Forward Founders and the Future of Companies

**Lenny:** You spend a lot of time with the most cutting-edge AI-forward founders. What are you seeing them do? How are they operating that's maybe blowing your mind about how the future of starting a company looks?

**Marc:** This is a great, very topical topic that's all playing out in real time right now on the leading edge. I think there are three layers to it.

### Layer One: AI Redefining Products

Layer one is they're thinking, "How does AI redefine the products themselves?" This is the time-honored thing that happens at technology transitions. There's a new technology that comes out—maybe it's the personal computer or the iPhone or the internet or now it's AI.

Is this a new capability that gets added to existing products? All of a sudden you've got an existing software business and now you got your PC version of it and now you got your iPhone version of it, and you kind of add the new technology into the mix—another ingredient into an existing formula.

A lot of new technologies are like that. When flash storage came out, you didn't really redefine the software industry—people just went from using hard disks to using flash storage.

But when the internet came out, old-school on-prem software for the most part died and just got replaced by web software. Sometimes you get "it's additive to an existing thing," sometimes you get "it redefines an entire product category, redefines an industry."

Take Adobe. Photoshop has built a 40-year franchise in image editing. Is AI a feature now that gets added to Photoshop to be able to do AI-based image editing? Or do you just stop editing images entirely because you're using tools like that and all images are just being generated, and it's just easier to have AI generate a new image than to try to edit an old one?

There are many areas of tech in which that question is being asked. Obviously as a venture firm, we're betting hard on many of these categories being totally reinvented, and a lot of the best founders are trying to figure out how to do that.

### Layer Two: AI Changing the Jobs

The next layer is AI changing the jobs. If I'm a founder of a company and I've got room in my budget for 100 coders, how do I get those coders to be superpowered AI coders, not the kind of coders I used to have? If they're superpowered AI coders, does that mean I still need the hundred? Maybe now I only need 10. Or does that mean I still want 100, but now they're doing 10 times more?

A lot of the best founders are working on that right now.

### Layer Three: Redefining the Company Itself

The third shoe to drop hasn't quite dropped yet, but it's the big one: Does the basic idea of having a company change?

You've got this concept of the superpowered individual. Can you have entire companies where the founder does everything because what the founder is doing is overseeing an army of AI bots?

There's this holy grail in our industry that's been running for a long time: can you have the one-person billion-dollar outcome? We've had a few of those over the years. Bitcoin is probably the most spectacular example, with Ethereum right behind it. Instagram and WhatsApp had very big outcomes with very small teams. Every once in a while you get one of these things where you have a very small number of people associated with it.

That said, most software companies obviously end up with huge numbers of employees. Some of the most leading-edge founders are thinking: How do I reconstitute the actual definition or idea of having a company? Can you have a company that's literally basically just all AI?

If you're doing anything in the real world, that's hard. But if you're doing software, that seems like it might be feasible in some cases.

Then there's the ultimate example: Can you have autonomous AI economy stuff happening where you have AI bots on the blockchain or something that are out there basically functioning as a business, making money, and just issuing dividends? Maybe that's the final outlier result. We have a few founders who are chasing that kind of thing.

**Lenny:** This whole idea of a one-person billion-dollar company—I think it depends on your definition. Having run my newsletter as one person with some contractors, there are so many little annoying things I have to deal with—support tickets, issues, bugs. It's hard for me to imagine actually a one-person billion-dollar company even if AI is handling so much of your support because there are just so many random edge cases.

**Marc:** Yeah. I mean, look, Bitcoin's Satoshi pulled it off. But like, the open-source community—does that count? I guess it counts. I don't propose to have answers here, but more just that the smartest people I know are thinking hard about this.

## Moats in AI

**Lenny:** What do you think about moats? That's a big question constantly in AI. What's your thesis on moats in AI? Is that even a thing?

**Marc:** My experience with really big technological transformations—and I lived this directly with the internet—is that they take a long time to play out, and there are all of these structural implications that cascade out over time.

There's this rush to judgment up front where people say, "Oh, it's therefore obvious that XYZ. It's obvious that this kind of company is going to be the company of the future, not that kind. It's obvious that this incumbent is going to be able to adapt and this other one isn't. It's obvious that there's economic opportunity in this kind of startup and not in these others. It's obvious that the moats are going to be in this area of the technology but not in this other area."

Everybody states those things with just an enormous amount of self-assurance where they really sound like they have all the answers. Then what happens is these ideas saturate the media because the media naturally prizes definitive answers over open questions. When CNBC is booking guests, they want a guest who's going to come on and say, "Yes, this is the way it's going to be," not "I think that's a really good question and let's debate it from eight different angles."

What I found is if you look back on those predictions a few years later—you can do this by pulling up coverage of the internet from 1993 through 1997 or even through 2005 or 2010 and looking at the kinds of confident statements people were making in the first 10 or 15 years—I would say almost all of them were wrong, generally quite badly wrong.

I think with massive technological change, it's going to be five or six layers of structural change that will play out over time. The implications on what are the definitions of products, companies, jobs, industries, how does this play out at the national level, the global level, how does this intersect with politics, unions, war, what's China going to do—there's just a tremendous number of unknowns, a very, very large number of unknowns.

I think it's really dangerous to prejudge these things.

### A Thought Experiment on AI Models

Let me run this as a thought experiment. Are AI models themselves defensible? Is there a moat on AI models?

On the one hand, you'd be like, "Wow, it certainly seems like there is or should be." If something takes billions of dollars to build, you need this incredible critical mass of compute and data, there's only a certain number of engineers in the world that know how to do this and they're getting paid like NBA stars, and these companies have to deal with all these crazy political issues and press issues and reputational stuff and regulatory and legal—all of that translates to "probably at the end of this there's going to be two or three companies that end up with market share and they're going to have profitability and it's going to be a classic oligopoly or maybe a monopoly." Those outcomes have happened in software many times before.

The other side: if you had told me three years ago that within basically a year to year and a half there would be five other American companies that would have basically equally capable products, and then there would be another five companies out of China that would have equally capable products, and then there would additionally be open source that was basically the same—I would have been like, "Wow, the thing that seemed like it was black magic all of a sudden has become commoditized really fast."

Which, by the way, is exactly what happened. Within a year of GPT-3 coming out, there were open-source GPT-3s running on a fraction of the hardware available for free. Now you've got fully in the game: Google, Anthropic, xAI, Meta, and all these other companies, and then DeepSeek and Kimi and all these other Chinese companies.

Even at the level of LLMs or AI models, you can squint and make that argument either way.

By the way, same thing at the level of apps. One school of thought is apps are not a thing because the model's just going to do everything. Another way of looking at it is: actually, adapting the model as the engine into a domain involving human beings—where you need to have it fit for purpose to be able to function in the medical industry or the legal industry or coding—you actually need the application level to matter enormously. Maybe the LLM's commoditizing, maybe the value goes to the apps.

I know very smart people who are on both sides of that argument. My honest answer is I think we're in a process of discovery over time. It's a complex adaptive system. The technology itself provides one of the inputs. The legal and regulatory process is another input. Actual individual choices made by entrepreneurs matter a lot. The economics matter a lot. Availability of investor capital varies over time—that matters a lot.

This is a complex system, and we actually don't know the outcomes on this yet. We need to be open to surprises at the structural level of what happens. As a VC, this is very exciting because it means we should make bets along every one of these strategies and see how this plays out.

There may be one particularly brilliant hedge fund manager or something who has this all figured out, but I guess I would say if they exist, I haven't met them yet.

### Don't Over-Obsess with Moats

**Lenny:** So what I'm hearing is don't over-obsess with moats at this point because we have no idea what it'll end up being.

**Marc:** Yeah. It's a little bit like—the holiday three years ago was the holiday of ChatGPT, and this last month has been the holiday of Claude, particularly Claude for coding. It's pretty amazing because there was Claude, which is obviously a great accomplishment, but then there's the coding agent, which is an app—it's a Claude wrapper, an agent harness.

Then they did this amazing thing where they came out with co-work. Remember they said co-work, which Claude wrote in a week and a half? There's two ways of looking at that. One: wow, that's really impressive that Claude was able to build co-work in a week and a half. That's amazing.

The other way to look at it is: co-work was developed in a week and a half. How much complexity could there be? How much of a barrier to entry can there be in something that was developed in a week and a half?

It's incredibly functional, incredibly valuable, and people all over the world every day are like, "Wow, I can't believe what I can do with this. This is the most magical product ever." But at the same time, it took a week and a half. Every other model company you'd have to expect is sitting there being like, "Obviously we need to build an agentic artist, and obviously we need to build a co-work thing for regular people."

How defensible is that? In six months, we've seen this happen before—is Claude going to get lapped the same way that GitHub Copilot got lapped? The history in the last three years has been everything that looks like it's the fundamental breakthrough gets basically replicated and lapped very quickly.

Many of the smartest people I know in the field, when I really talk to them and get a couple drinks into them, they're like, "Yeah, basically one theory is there really aren't any secrets among the big labs." The big labs kind of all have the same information and knowledge. They lap each other on a regular basis, but there's not a lot of proprietary anything at this point.

Evidence of that: DeepSeek came out of left field and was basically a re-implementation of a lot of the ideas under American big labs, and it had some original ideas of its own. But wow, it wasn't that hard for basically a hedge fund in China to do it. So how much defensibility is there?

On the other side, you've got all these big labs now paying individual engineers like they're rock stars, and they're incredibly bright and creative people. Maybe there's a dozen nascent ideas in any one of these labs that's actually going to be a huge breakthrough that's going to be hard to replicate.

Again, I think we just need to—my view is I need to put a big discount on my forecasting ability on this one. It's much less interesting to try to say, "Okay, as a consequence, industry structure in five years is going to be X, the big winner in the category is going to be company Y, the killer app is going to be Z." I don't think I can predict that.

I think a much better use of my time is being very flexible and adaptable at a time like this.

### Indeterminate Optimism

You remember the Peter Thiel formulation? There's a two-by-two: optimism and pessimism, and then there's determinate and indeterminate. He always argued that Silicon Valley is characterized by too much what he calls "indeterminate optimism."

What he meant by that is basically an indeterminate optimist thinks the world is going to be better but can't explain why. Some combination of things is going to happen to make the world be better even if we don't know what those things are. He would say that risks being wishful thinking or delusional thinking.

What the world needs more is determinate optimists, which are people who are like, "No, the world is going to be better because I'm going to do this specific thing." He would classify Elon as the determinate optimist. He would sort of say VCs are indeterminate optimists. Elon is like, "No, I'm going to build the electric car, and I'm going to do solar, and then I'm going to do Mars"—very concrete things.

I think there's a lot to Peter's framework, but the way I would describe it is I think the indeterminate optimism is a stronger phenomenon than at least I think he's historically represented it as. I would put myself firmly in the indeterminate optimist category, and that's the strategy that we have at a16z.

The indeterminate optimism of venture capital or of Silicon Valley is very specific: there are these extremely bright and capable people like Elon and many others who are founders and product creators. Each of those individual people is a determinate optimist. Each of them individually has a very strong view of what they're going to do.

But the great virtue of the capitalist system, the great virtue of the American economy, the great virtue of Silicon Valley, is we don't just have one of those and we don't just have 10 of those. We have a hundred and a thousand and then 10,000 of those. The way to optimize the outcome is to have as many of those as possible be as good as possible, run as hard as possible.

The nature of the future is we just don't know all the answers, and that's okay. The right way to deal with that is to run as many experiments as possible and have as many smart people try to do as many interesting things as possible.

I would put myself firmly on the side of the indeterminate optimist.

**Lenny:** I'm wondering if the answer to the question of what you look for now more and more is this determinate optimistic founder that has this massive ambition and is actually working on achieving it.

**Marc:** Yeah, that's right. The founders need to be determinate optimists. They need to have a very specific plan.

The critique from the founders is always, "Oh, VCs have it easy because you don't actually have to commit. You don't actually have to make the bed you lay in. You can place multiple bets, you can operate a portfolio. You should have a lot more sympathy for us as founders because we only get to make the one bet."

There's truth to that. The counter-argument is the founders get to run their companies. We don't. We don't get to put our hand on the steering wheel. The great virtue of being a determinate optimist is you actually get to single-mindedly execute against that goal.

In the long run, who does history remember? History remembers Henry Ford, not whoever was the seed investor who seeded Ford Motor Company and 10 other car companies that failed.

The determinate optimist—the founder, the company builder, the engineer—these are the people who actually do the thing and deserve 99.99999% of the credit. But having said that, I do think there is a role for having some indeterminate optimists in the background helping along the way and helping keep the whole cycle going.

## AGI and the Limits of Human Capability

**Lenny:** Do you think about AGI in shifting your investment thesis? As we approach AGI and hit AGI as an investor, how do you think about your investment thesis changing?

**Marc:** I've always kind of struggled with the concept of AGI because there are two defined terms. There's the prosaic definition of AGI and then there's the cosmic definition.

### The Cosmic Definition

The cosmic one is basically the singularity. AGI is the moment where you enter the singularity, which is to say the world fundamentally changes and the rules of the old world are gone. We're now operating in a new domain.

The full definition of singularity is it's a world in which human judgment is no longer really relevant because you get this self-improvement loop. The AI is improving itself and it's racing—so-called takeoff scenarios. The AI is improving itself and the machines are making decisions so much faster than people, and people are just sitting there watching the machine do its thing.

I don't really think we live in that world. Whether you could call that utopian or dystopian, I don't think we're lucky or unlucky enough to live in that world.

### The Prosaic Definition

The prosaic definition of AGI that the industry participants have converged on is: it's when the AI can do every economically relevant task as good as a human. Or as the co-founder of Anthropic put it: a basket of the most valuable economic tasks. So it's 10-15, not every single economically valuable task.

**Lenny:** And by the way, we're clearly getting close to that if we're not already there.

**Marc:** I kind of feel like the cosmic one overstates what's going to happen, and I kind of feel like the AGI definition you just gave understates what's going to happen. It's almost too reductionist.

The reason for that is I don't think there's any reason to assume that human skill level is the cap on anything. AGI is always defined kind of relative in comparison to a human worker. But human skill level caps out at a certain point because of the inherent biological limitations of the human organism.

### Beyond Human Intelligence

Human IQ—what they call fluid intelligence or the G factor of fluid intelligence—tops out in humans as a species around 160. At 160, it's Einstein level, Feynman IQ. The 160 IQ people are the ones who come up with new physics. There's only a small handful of those.

Generally speaking, when we run into somebody in the world who's incredibly smart—a best-selling author or one of the world's best research scientists or doctors—it's probably 140. If you're looking for a really good lawyer, it's probably 130. A really good line manager in a business is probably 110. An accountant, a small business accountant who's good at doing the books for small businesses, is probably 105.

The scope of impressive human ability to do intellectually impressive things is that 110 to 160 spectrum. There are a lot of those people running around, but not that many at 140, 150, 160. That's just the limitations of what can fit in here. There's no theoretical limit on where this goes if you release the limitations of human biology.

People are already running experiments to do human-equivalent IQ for existing AI models. By the way, existing AI models right now are testing around the 130-140 level, which means they're going to get to the 160 level. They're arguably on the math side starting to get to the 160 level now.

But I think we're going to have AI models relatively quickly that are going to be 160, 180, 200, 250, 300, by the way. And I think that's great. I feel as great about that as I do about the fact that we occasionally get an Einstein. Would the world be better off or worse off with more or fewer Einsteins? The answer is of course the world would be better off with more Einsteins.

Of course the world would be better off with machines that have IQ equal to or greater than Einstein. I think the IQ of the machines is going to exceed that of humans, and I think that's really good.

The performance against tasks is going to get better also. This is where Linus Torvalds in particular is like, "Yeah, okay, this thing is starting to generate better code than I can." We're going to have AI coders that are actually better coders than the best human coders. I think that's great.

I think we're going to have AI doctors that are better than the best human doctors. I think we're going to have AI lawyers that are better than the best human lawyers, which is going to be very interesting to see.

I don't think there's a cap. I think we're used to living in a world where we just don't understand how good "good" can get because we've been capped by our own biology. We're going to get to experience what it's like when you have the capability at your fingertips that's actually better than human in these domains.

This idea of "human equivalent" is just going to be a footnote. It's like, "Oh yeah, that was just on Tuesday in 2026 when they hit that," and it kind of didn't matter because the next question was, "What are we going to do in a world in which we actually have machines that are better than that?"

This is going to be much more of an exploratory process for actually exceeding human capability than it's going to be any sort of particular singularity moment that just happens to coincide with the human threshold.

### The Experience of Limitation

**Lenny:** 200 IQ. That frame of reference is such a mind-expanding way to think about just how fast and how smart these things are going to get.

**Marc:** I don't know if you have this experience, but I have this experience all the time. I'm just like, "I know I ought to be able to do this, but I just can't. It's going to take too long." I want to write this thing or I want to have this theory on this thing or I have a plan. It's just like I don't have the eight hours or the eight weeks or the eight years. I just don't know enough yet. I just can't do the math in my head. My memory isn't perfect. I can't remember. You read 10 books and then you're like, "I forgot almost everything that I just read."

I wish I could retain it all, but I can't. I sort of live in this state of endless frustration. It's like if I could just be smarter than I was, I'd be so much better at what I do, but I'm not.

I have this on a regular basis: Because of what we do, I know a bunch of people who I know for sure are smarter than I am. I know it because when I talk to them, I just find myself at a certain point—for the first half of the conversation, I'm just taking notes the entire time. For the second half of the conversation, I'm just like, "This person is just smarter than I am and they're just outthinking me and they're going to keep outthinking me." I'm just like, "All right, goddammit, I gotta go home and I gotta have a drink because I'm just not that."

We're so used to having those limitations that the idea of having machines that work for us that don't have those limitations—I just think that's much more exciting than people are giving credit for.

## Media and Product Diet

**Lenny:** In terms of media diet, what are you reading? What are you paying attention to these days in terms of podcasts, newsletters, blogs? And any books in particular?

**Marc:** I read basically three categories of things. I have almost a perfect barbell strategy: I read X (Twitter) and I read old books. It's basically either up-to-the-minute what's happening right now, or it's a book that was written 50 years ago that has stood the test of time and presumably there's something timeless in it.

Everything in the middle I'm always much more skeptical about. It's what I already said: if you go back and you read old newspapers—and by the way, nobody ever does this, there's no market for it—but if you go back and read last week's newspaper, just go back and read it and be like, "Oh my god, none of this happened."

None of what they predicted played out the way they said it would. None of this turned out to actually be that relevant or correct. They had no view of what was going to happen this week that they couldn't know, so they were making predictions and forecasts based on not having any information. But it's like, "Wow, none of this happened. I wish I had never read this."

It's the same thing with magazines. Go back and read old magazines and just the endless numbers of predictions that they make. At least newspapers are going day-to-day. The thing with magazines is it's a week or month-long cycle, so by the time an article even hits publication, it's often out of date.

I just have a big problem with everything in the middle. It's either of the moment or timeless.

### Domain Practitioners

The other thing—and this is maybe obvious, but I think it's probably still underrated—is the actual practitioners in the field who are actually creating content. I think this is probably still dramatically underrated. I think this is a huge part of the Substack phenomenon and the newsletter phenomenon and the podcast phenomenon: direct exposure to the people who are actually principals in the field who actually know what they're talking about is probably still dramatically underrated.

The reason for that is we're used to being in this mass media culture in which basically everything is mediated. Everything got filtered through TV interviews or newspaper interviews or magazine interviews. Now more and more it's just "you actually want smart people who are actually working on something explaining themselves."

You have new kinds of intermediation like podcasts that open that up for people to make that possible. Domain practitioners are really great. In AI, obviously your stuff, Lenny, but also Lex Friedman can have the world's leading experts in the domain actually show up.

The critique always is "people talk their book"—if I'm running a startup, I'm just selling. There's always a little bit of that. But my experience is people love to talk about what they do, and they fundamentally want to express what they do and they want people to understand it. Everybody kind of enjoys that. They get to contribute to human knowledge by doing that, and they get ego gratification by doing that.

There's just actually tremendous amounts of alpha in listening to the world's leading experts in the space who actually just show up and talk about what they're doing. The world is awash in that today in a way that it wasn't as recently as 10 years ago.

**Lenny:** There's also this culture in tech, Silicon Valley in particular, of sharing, of not trying to keep these secrets. Everyone on LinkedIn is always like, "How is this free?"

**Marc:** Yeah. Somebody said Silicon Valley is a company town, but the company is Silicon Valley.

At the level of running a company, that's just a giant pain in the butt because your secrets are walking out the door and your employees are walking out the door. But the other side of it is you also benefit from that because you get to hire people with all these skills and experiences. You're in this ecosystem that adapts and channels talent and skill and knowledge and people into the new fields.

There's the push and pull of that at the level of just being an individual CEO. At the level of just being in the ecosystem, it's an absolutely magical phenomenon.

One of the things—for all the issues in Silicon Valley—AI is the ninth major technology platform in the history of Silicon Valley. Silicon Valley is still called Silicon Valley. We haven't made silicon here in decades. We used to actually make chips. They used to have the actual fabs in Silicon Valley, and they designed them and made the chips. That was wave one starting in the 1950s. But now we're on wave nine.

The company town phenomenon where the company is the industry—the indeterminate optimism—nobody had to sit and plan and say, "Okay, in the 1990s Silicon Valley is going to do the internet, in the 2000s they're going to do the smartphone, in the 2010s they're going to do the cloud, in the 2020s they're going to do AI." The flexibility of the ecosystem meant that Silicon Valley could morph into all these categories. Again, maybe a testimony to indeterminate optimism.

### Favorite Movie: Edington

**Lenny:** Ben Horowitz said you're really into movies these days. Any movies you've absolutely loved recently?

**Marc:** The movie that blew my socks off last year, which I think is the best movie of the decade for sure and maybe of the last 15 years, is this movie called Edington. Unfortunately, not a lot of people have seen it, but I would highly encourage it.

At the surface level, it's set in a small town in New Mexico called Edington, a town of about 600 people. There's a sheriff played by Joaquin Phoenix, who's like an old crusty basically right-winger. Then there's a mayor played by Pedro Pascal who's basically a young hip progressive.

The movie starts I think in March of 2020. It starts when COVID first hits, and then as it plays out over the next few months, it intersects and extends into the summer of 2020. So you get the George Floyd moment and then the protests and riots—the convergence of COVID and then all the BLM stuff.

Then there's a third element: there's a company which is basically a loosely disguised version of Meta (if you read the backstory) which is building an AI data center on the outskirts of town. They kind of pull that in as a thing that looms larger and larger over time.

The thing it really is great at is it really shows—this is a small town in New Mexico, and everybody in the town gets fully wrapped up in all the COVID stuff and they get fully wrapped up in all the BLM stuff and they get fully wrapped up in all the tech anxiety stuff, but they're all experiencing it basically through the internet, which is what actually happened.

The reason I love the movie so much: one, it's the first movie that directly grapples with 2020—what happened in 2020—and just fully engages and grapples with all the dynamics that were playing out in the country. The other reason is it's the first movie that does a really good job of showing what it was like, especially in that era, to live in a world in which things happen in the real world and people were experiencing events online in a way that was very central in their lives.

It does a really good job of pulling in smartphones and social media in a way that movies really struggle with. The whole thing comes together in an incredibly entertaining way.

I won't even say I completely agree with the movie, and I think the director and I would probably disagree about a lot. But he really tries hard to really grapple with what it's actually like to live like a human being in the 2020s in America in a way that I think many other filmmakers who are very talented have just been very scared of touching. This guy, for some reason, is just like, "Yeah, I'm just going to find all the third rails and I'm just gonna grab them."

### Product Recommendations

**Lenny:** Are there any products you use that maybe are less known that you love that you want to recommend?

**Marc:** We have so many that it's really hard. It's like, "Who's your favorite children?" But I'll talk about a few.

One is my 10-year-old right now is 100% obsessed with Replit. And by the way, it was not from me. Do you have kids?

**Lenny:** I do. I have one two-and-a-half-year-old.

**Marc:** Two and a half. Okay. So you haven't run into what I'm running into now, which is whatever it is you do is not cool. At two and a half, whatever daddy does is the coolest thing in the world. I can tell you by the time he's 10, whatever you do is deeply uncool, and I'm highly aware of that.

If I mention, "Oh yeah, we work on XYZ," he's like, "Okay." But when he discovers something, then it's cool. Or when his friends tell him about it, it's cool.

He through no interference on my part discovered Replit about three months ago and discovered vibe coding, and is completely obsessed with vibe coding games and all kinds of things. He was literally doing it for hours, and so I'm seeing that phenomenon play out, which is super fun.

Two is I am just completely in love with all the AI voice stuff. I think it's just absolutely amazing, hysterical. My favorite party trick at dinner parties now is to pull out Grok with Bad Rudy, which is a foul-mouthed raccoon avatar in the Grok app. I think that's super fun.

We have this company Sesame that went viral last year for this just incredibly intimate, emotional voice experience. I think the voice stuff is fantastic.

I'm also super fascinated by all the voice input stuff. The pendants, the wearables—all that stuff is going to be big. The Meta glasses—I think there's going to be a whole wearables revolution here.

I have this app on my phone now called Whisper Flow, which is voice transcription. It works staggeringly well. It's a voice transcription function, but you can actually talk to the AI model while you're doing voice transcription. It kind of understands when you're telling it, "No, no, I want bullet points over there and I want this and that." It understands that you're not telling it to type in the words "I want bullet points"—it just actually understands that you want bullet points. That's a great example of a super useful thing.

I think the voice mode stuff is going to be really great.

**Lenny:** Subscribers of my newsletter get a year free of Replit and Whisper Flow. What's the most memorable thing your son built with Replit?

**Marc:** Oh, well, he's gotten super into Star Trek. So far it's been he's writing Star Trek simulators.

**Lenny:** Next generation?

**Marc:** Next generation. We actually like them all. We watched the new Starfleet Academy last night, which actually is quite good. We watched the original, we watched them all, but it was in Next Generation where they actually developed an actual design language for the computers.

If you watch the original series, they just had basically knobs with lights and they didn't really—they just were like pretending on set. But by Next Generation, they actually had designed a UI design language.

One of the fun things you can do with vibe coding is you can say, "Give me a Star Trek Next Generation user interface for whatever." And it actually uses—I'm a nerd now—they call it the LCARS design language, and it'll actually build you Star Trek Next Generation interfaces using that design language but with your choice of a Star Trek game, for example. He's going crazy for that kind of thing.

**Lenny:** That sounds extremely delightful. You guys should open-source or release that.

## Closing Thoughts

**Marc:** A couple things. One is we got super lucky last week. Puck McCormack wrote the best piece ever written about us, which is the best explanation of what we do and how we think. I would definitely recommend that.

We're putting a lot of effort ourselves into video, into content. I definitely recommend our YouTube channel, which I think has a lot of great stuff and is going to be very exciting in the next year.
